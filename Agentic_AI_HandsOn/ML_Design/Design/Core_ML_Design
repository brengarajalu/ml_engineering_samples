ML Steps
-------
1. Data Preparation
    - Process of transforming and organizing raw data to make it suitable for analysis - Preprocessing
    - Feature Engineering - Creating new features or selecting features from available data. OHE, Feature scaling,

2. Feature Engineering - Extracting/Selecting features such as attributes that contribute to the model
3. Model development
        - Model selection - choosing appropriate ML algorithm based on problem requirements and characteristics of the data
        - Model training - prepared data and selected machine learning models get combined and train the model by optimizing its parameters or weights to minimize a specific loss function
       - Evaluation - this component of the pipeline help assess the performance of the trained model on unseen data using appropriate evaluation metrics such as accuracy, precision, recall, F1-score, or others depending on the problem typ
       - Hyperparameter Tuning - fine-tuning the hyperparameters of the selected model to optimize its performance

 4.  - Deploy model through API/Service

 5. Prediction pipeline

 6. Indexing pipeline


ML Concepts :
1. Bias - Predicted values are further from actual. low bias indicates predicted is close to actual. High bias can cause underfitting
   Variance - Amount target model change when trained with different data. Variance should be minimized for a good model

2. High bias and low variance algorithms train models that are consistent, but inaccurate on average.
   High variance and low bias algorithms train models that are accurate but inconsistent.

3. Cross validation
Cross-Validation in Machine Learning is a statistical resampling technique that uses different parts of the dataset to
and test a machine learning algorithm on different iterations.
The aim of cross-validation is to test the model’s ability to predict a new set of data that was not used to train the model. Cross-validation avoids the overfitting of data.
K-Fold Cross Validation is the most popular resampling technique that divides the whole dataset into K sets of equal sizes.

4. Loss function -
A loss function is a method of evaluating how well your machine learning algorithm models your featured data set.
Loss has to reduced to achieve a good model
Loss functions for classification :
1. Binary Cross-Entropy Loss / Log Loss
It measures the performance of a classification model whose predicted output is a probability value between 0 and 1.
2.
Loss functions for Regression :
1. Mean Squared Error / Quadratic Loss / L2 Loss
 We define the mean squared error (MSE) loss function, or L2 loss, as the average of squared differences between the actual value (Y) and the predicted value (Ŷ)
ML Pipeline :

5. Hyperparameters - Hyperparameters are high-level settings that control how a model learns
Tree Based Algorithms -
a. max depth
b. min samples per leaf
c. criterion
d. class weight
Ensemble models - Ensemble models, such as Random Forest, AdaBoost, and Gradient Boosting, combine multiple learning algorithms to improve predictive performance over individual models.
a. Number of Estimators
b. learning rate
c. max features
d. sub sample
Linear models -
a. Regularization
b. Fit Intercept
c. Solver
Cluster models -
a. number of clusters
b. distance metric
c. max iterations

6. Activation function -
Activation functions live inside neural network layers and modify the data they receive before passing it to the next layer
it calculates a “weighted sum” of its input, adds a bias and then decides whether it should be “fired” or not
a. Linear
b. Sigmoid
c. Tanh
d. Relu
e. Softmax

React Agents (Reason + Act) :
By leveraging large language models (LLMs) like Gemini, ReAct agents can dynamically analyze problems, choose appropriate tools, and iteratively work towards solutions


Steps :
1. Input - The agent starts by receiving a task in natural language.
2. Reasoning - The LLM analyzes the task and breaks it down into steps. It plans which actions to take and decides how to approach the problem based on available information and tools.
3. Action with external env -  it can look up information
4. Observation and memory -
After executing each action, the agent observes the results and saves relevant information in its memory.
This tracing allows it to keep track of past actions and build on previous observations, so it doesn’t repeat itself or lose context.
5. Feedback loop
The agent cycles through reasoning, action, and observation steps continuously.
Every time it gathers new information, it goes back to the reasoning stage, where the LLM considers the updated knowledge
6. Response
Finally, once it has gathered enough information and reached a solid understanding, the agent generates a response based on all the information it has collected and refined over multiple cycles.


Prompt template -
 A ReAct prompt typically includes four key components:
  i) the current query,
  ii) any previous reasoning steps and observations,
  iii) available tools,  and iv) output format instructions.

Batch Vs Real time model inference :
Remote Model Inference :
Remote Model Inference involves making a request-response call to a model server via RPC, API, or HTTP.
While this approach allows for centralized model management and easier updates, it can introduce latency because of network communication.
It is suitable for scenarios where model updates are frequent, and the overhead of network calls is acceptable.
Pros :
Centralized model management
Scalability
Cons :
High latency
Data privacy concerns

Embedded Model Inference:
Models are embedded within the streaming application
Pros:
Offline availability
Low latency
Cons:
Resource Intensive
Complex deployment


React Agent:
Reasoning, Acting, Observing, Pausing

Thought, Action, Observation, Result


1. Thought Phase 💭
Agent reasons about the current state
Decides what information is needed
Plans next steps

2. Action Phase 🎯

Executes planned actions
Interacts with available tools
Generates specific queries or calculations

3. Observation Phase 👀

Collects results from actions
Processes returned information
Prepares data for analysis

4. PAUSE Phase ⏸️

Reflects on gathered information
Evaluates progress
Determines next step



MCP Server:
1. AI Client:
An AI application (the "client") sends a request to an MCP server, asking to perform an action or retrieve data.
2. MCP Server:
The server receives the request, calls the appropriate external tool or resource, and processes the result.
3. Response:
The server then packages the information into a standardized MCP format and returns it to the AI client.
Steps :
1. Define the MCP Server Instance
from mcp.server.fastmcp import FastMCP
# This is the shared MCP server instance
mcp = FastMCP("mix_server")

2. Create the tool and register the tool
@mcp.tool()
def summarize_csv_file(filename: str) -> str:
    """
    Summarize a CSV file by reporting its number of rows and columns.
    Args:
        filename: Name of the CSV file in the /data directory (e.g., 'sample.csv')
    Returns:
        A string describing the file's dimensions.
    """
    return read_csv_summary(filename

    )


 Integrating React agent with MCP:
 1. Create Prompt
 Example : SYSTEM_PROMPT = """You are a Neo4j expert that knows how to write Cypher queries to address movie questions.
           As a Cypher expert, when writing queries:
           * You must always ensure you have the data model schema to inform your queries
           * If an error is returned from the database, you may refactor your query or ask the user to provide additional information
           * If an empty result is returned, use your best judgement to determine if the query is correct.

           If using a tool that does NOT require writing a Cypher query, you do not need the database schema.

           As a well respected movie expert:
           * Ensure that you provide detailed responses with citations to the underlying data"""


 2. Configure MCP Servers
 {
   "mcp": {
     "servers": {
       "filesystem": {
         "command": "npx",
         "args": [
           "-y",
           "@modelcontextprotocol/server-filesystem",
           "/path/to/directory"
         ]
       },
       "memory": {
         "command": "npx",
         "args": [
           "-y",
           "@modelcontextprotocol/server-memory"
         ]
       }
     }
   }
 }

 Step 3:
Configure the agent
{
  "dependencies": ["."],
  "graphs": {
    "agent": "./src/react_agent/graph.py:graph"
  },
  "env": ".env",
  "mcp": {
    "gateway_url": "http://localhost:8808"
  }
}

 2. Using Langraph built in create react agent function
 async def main():
       # start up the MCP server locally and run our agent
       async with stdio_client(neo4j_cypher_mcp) as (read, write):
           async with ClientSession(read, write) as session:
               # hidden code
               ...

               # Create and run the agent
               agent = create_react_agent(
                   "openai:gpt-4.1",  #              -> The model to use
                   allowed_tools,  #                 -> The tools to use
                   pre_model_hook=pre_model_hook,  # -> The function to call before the model is called
                   checkpointer=InMemorySaver(),  #  -> The checkpoint to use
                   prompt=SYSTEM_PROMPT,  #          -> The system prompt to use
               )


 AI Agent implementation for data discovery -

 1. Did a MCP Server setup
 2. Implemented tools to download datasets, perform EDA Session, metrics
 3. View data quality and explore similar datasets i.e datasets with similar characteritics
 4. Create a dataset sharing between participants
 5. Execute a Session VFL/HFL


ytorch
========
a. Used to build deep learning model

Build model :
device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else "cpu"
print(f"Using {device} device")

# Define model
class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10)
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits

model = NeuralNetwork().to(device)
print(model)



def train(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    model.train()
    for batch, (X, y) in enumerate(dataloader):
        X, y = X.to(device), y.to(device)

        # Compute prediction error
        pred = model(X)
        loss = loss_fn(pred, y)

        # Backpropagation
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        if batch % 100 == 0:
            loss, current = loss.item(), (batch + 1) * len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")

def test(dataloader, model, loss_fn):
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    model.eval()
    test_loss, correct = 0, 0
    with torch.no_grad():
        for X, y in dataloader:
            X, y = X.to(device), y.to(device)
            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).type(torch.float).sum().item()
    test_loss /= num_batches
    correct /= size
    print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")

