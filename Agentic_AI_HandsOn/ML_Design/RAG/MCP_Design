MCP
=====
The Model Context Protocol (MCP) is an open, standardized interface that allows Large Language Models (LLMs) to interact seamlessly with external tools, APIs, and data sources.

MCP follows a client-server model, where:

MCP Hosts run the AI models
MCP Clients initiate requests
MCP Servers serve context, tools, and capabilities

MCP servers operate in the following way:

Request Flow:
A request is initiated by an end user or software acting on their behalf.
The MCP Client sends the request to an MCP Host, which manages the AI Model runtime.
The AI Model receives the user prompt and may request access to external tools or data via one or more tool calls.
The MCP Host, not the model directly, communicates with the appropriate MCP Server(s) using the standardized protocol.
MCP Host Functionality:
Tool Registry: Maintains a catalog of available tools and their capabilities.
Authentication: Verifies permissions for tool access.
Request Handler: Processes incoming tool requests from the model.
Response Formatter: Structures tool outputs in a format the model can understand.
MCP Server Execution:
The MCP Host routes tool calls to one or more MCP Servers, each exposing specialized functions (e.g., search, calculations, database queries).
The MCP Servers perform their respective operations and return results to the MCP Host in a consistent format.
The MCP Host formats and relays these results to the AI Model.
Response Completion:
The AI Model incorporates the tool outputs into a final response.
The MCP Host sends this response back to the MCP Client, which delivers it to the end user or calling software.


MCP Security :
1. Use OAuth token auth
    a. MCP Server acts as resource server. MCP client gets the auth server info from resource server
    b. mcp client requests token from auth server and gets the token
    c. access resource server with the token
2. Use RBAC


Context management:
Document Chunking -	Splits documents into smaller, coherent chunks to preserve context while reducing redundancy and staying within LLM limits.
Selective Retrieval	Filters large sets of relevant documents to retrieve only the most pertinent parts, minimizing extraneous information.
Targeted Retrieval	- Optimizes retrieval for specific query intents using specialized retrievers, adding domain-specific criteria to refine results.
Context Summarization -	Uses extractive or abstractive summarization techniques to condense large amounts of retrieved content, ensuring essential information is passed to the LLM.


Streamable HTTP:
================
Improvements over traditional HTTP+SSE as
Key Advantages of Streamable HTTP
Technical Advantages
Simplified Implementation: Works on standard HTTP servers without special requirements
Resource Efficiency: Allocates resources on demand, avoiding long-lived connections per client
Infrastructure Compatibility: Works well with existing web infrastructures (CDNs, load balancers, API gateways)
Horizontal Scalability: Supports routing requests to different server nodes via message buses
Progressive Adoption: Service providers can choose implementation complexity based on needs
Reconnection Support: Enables session recovery for improved reliability
Business Advantages
Reduced Operational Costs: Lowers server resource consumption and simplifies deployment architecture
Enhanced User Experience: Improves experience through real-time feedback and reliable connections
Broad Applicability: Suitable for everything from simple tools to complex AI interactions
Scalability: Supports more diverse AI application scenarios
Developer-Friendly: Lowers the technical barrier to MCP implementation


What Is Context?
The Context object provides a clean interface to access MCP features within your functions, including:
Logging: Send debug, info, warning, and error messages back to the client
Progress Reporting: Update the client on the progress of long-running operations
Resource Access: List and read data from resources registered with the server
Prompt Access: List and retrieve prompts registered with the server
LLM Sampling: Request the clientâ€™s LLM to generate text based on provided messages
User Elicitation: Request structured input from users during tool execution
State Management: Store and share data between middleware and the handler within a single request
Request Information: Access metadata about the current request
Server Access: When needed, access the underlying FastMCP server instance



React Agent:
Reasoning, Acting, Observing, Pausing

Thought, Action, Observation, Result


1. Thought Phase ðŸ’­
Agent reasons about the current state
Decides what information is needed
Plans next steps

2. Action Phase ðŸŽ¯

Executes planned actions
Interacts with available tools
Generates specific queries or calculations

3. Observation Phase ðŸ‘€

Collects results from actions
Processes returned information
Prepares data for analysis

4. PAUSE Phase â¸ï¸

Reflects on gathered information
Evaluates progress
Determines next step



MCP Server:
1. AI Client:
An AI application (the "client") sends a request to an MCP server, asking to perform an action or retrieve data.
2. MCP Server:
The server receives the request, calls the appropriate external tool or resource, and processes the result.
3. Response:
The server then packages the information into a standardized MCP format and returns it to the AI client.
Steps :
1. Define the MCP Server Instance
from mcp.server.fastmcp import FastMCP
# This is the shared MCP server instance
mcp = FastMCP("mix_server")

2. Create the tool and register the tool
@mcp.tool()
def summarize_csv_file(filename: str) -> str:
    """
    Summarize a CSV file by reporting its number of rows and columns.
    Args:
        filename: Name of the CSV file in the /data directory (e.g., 'sample.csv')
    Returns:
        A string describing the file's dimensions.
    """
    return read_csv_summary(filename

    )


 Integrating React agent with MCP:
 1. Create Prompt
 Example : SYSTEM_PROMPT = """You are a Neo4j expert that knows how to write Cypher queries to address movie questions.
           As a Cypher expert, when writing queries:
           * You must always ensure you have the data model schema to inform your queries
           * If an error is returned from the database, you may refactor your query or ask the user to provide additional information
           * If an empty result is returned, use your best judgement to determine if the query is correct.

           If using a tool that does NOT require writing a Cypher query, you do not need the database schema.

           As a well respected movie expert:
           * Ensure that you provide detailed responses with citations to the underlying data"""


 2. Configure MCP Servers
 {
   "mcp": {
     "servers": {
       "filesystem": {
         "command": "npx",
         "args": [
           "-y",
           "@modelcontextprotocol/server-filesystem",
           "/path/to/directory"
         ]
       },
       "memory": {
         "command": "npx",
         "args": [
           "-y",
           "@modelcontextprotocol/server-memory"
         ]
       }
     }
   }
 }

 Step 3:
Configure the agent
{
  "dependencies": ["."],
  "graphs": {
    "agent": "./src/react_agent/graph.py:graph"
  },
  "env": ".env",
  "mcp": {
    "gateway_url": "http://localhost:8808"
  }
}

 2. Using Langraph built in ceate react agent function
 async def main():
       # start up the MCP server locally and run our agent
       async with stdio_client(neo4j_cypher_mcp) as (read, write):
           async with ClientSession(read, write) as session:
               # hidden code
               ...

               # Create and run the agent
               agent = create_react_agent(
                   "openai:gpt-4.1",  #              -> The model to use
                   allowed_tools,  #                 -> The tools to use
                   pre_model_hook=pre_model_hook,  # -> The function to call before the model is called
                   checkpointer=InMemorySaver(),  #  -> The checkpoint to use
                   prompt=SYSTEM_PROMPT,  #          -> The system prompt to use
               )


 AI Agent implementation for data discovery -

 1. Did a MCP Server setup
 2. Implemented tools to download datasets, perform EDA Session, metrics
 3. View data quality and explore similar datasets i.e datasets with similar characteritics
 4. Create a dataset sharing between participants
 5. Execute a Session VFL/HFL